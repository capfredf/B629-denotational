\documentclass{tufte-handout}
%\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{semantic}
\usepackage{wrapfig}
\usepackage{xypic}
%\usepackage[noanswer]{exercise}
\usepackage[answerdelayed]{exercise}
\usepackage{upgreek}

\renewcommand{\ExerciseHeader}{\noindent \textbf{
                \ExerciseName\;\ExerciseHeaderNB\ExerciseHeaderTitle
                \ExerciseHeaderOrigin}}

\renewcommand{\AnswerHeader}{\medskip{\noindent\textbf{Answer of \ExerciseName\ \ExerciseHeaderNB \\[1ex]}\smallskip}}

%\newcommand{\BR}[1]{\llbracket #1 \rrbracket}
\newcommand{\defeq}[0]{\overset{\mathrm{def}}{=}}
\newcommand{\BR}[1]{(#1)}
\newcommand{\LAM}[1]{\lambda #1.\,}
\newcommand{\APP}[0]{\,}
\newcommand{\ASSIGN}[2]{#1 \mathrel{:=} #2}
\newcommand{\SEQ}[2]{#1 \mathrel{;} #2}
\newcommand{\OUTPUT}[1]{\mathtt{put}(#1)\texttt{;}}
\newcommand{\SKIP}[0]{\mathtt{skip}}
\newcommand{\INPUT}[0]{\mathtt{get}()}
\newcommand{\IF}[3]{\mathtt{if}\,#1\,\mathtt{then}\,#2\,\mathtt{else}\,#3}
\newcommand{\WHILE}[2]{\mathtt{while}\,#1\,\mathtt{do}\,#2}
\newcommand{\TRUE}[0]{\mathtt{true}}
\newcommand{\FALSE}[0]{\mathtt{false}}
\newcommand{\of}[0]{\!:\!}
\newcommand{\pto}[0]{\rightharpoonup}
\newcommand{\NAT}[0]{\ensuremath{\mathit{Nat}}}
\newcommand{\COND}[3]{#1\,\texttt{?}\,#2\,\texttt{:}\,#3}

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}%[section]
\newtheorem{conjecture}{Conjecture}%[section]
\newtheorem{example}{Example}%[section]
\newtheorem{remark}{Remark}

\title{Lecture Notes on Denotational Semantics}
\author{Jeremy G. Siek}

\begin{document}

\maketitle

\tableofcontents

\clearpage


\section{Binary Arithmetic}
\label{sec:binary-arithmetic}

\marginnote{
\begin{tabular}{ll}
Reading: & \citet{Schmidt:1986vn} Chapter 4 \\
Exercises: & 4.2, 4.6
\end{tabular}
}

Borrowing and combining elements from Chapter 4 of
\citet{Schmidt:1986vn}, we consider the language of binary arithmetic
specified in Figure~\ref{fig:binary-arithmetic}.  The Syntax defines
the form of the programs and the Semantics specifies the behavior of
running the program. In this case, the behavior is simply to output a
number (in decimal).  Our grammar for binary numerals departs slightly
from \citet{Schmidt:1986vn}, with $\epsilon$ representing the empty
string of digits.

The main purpose of a semantics is communicate in a precise way with
other people, primarily language implementers and programmers.  Thus,
it is incredibly important for the semantics to be written in a way
that will make it most easily understood, while still being completely
precise.  Here we have chosen to give the semantics of binary
arithmetic in terms of decimal numbers because people are generally
much more familiar with decimal numbers. 

\begin{figure}
\noindent Syntax
\[
\begin{array}{lrl}
 \text{digit}& d ::=& 0 \mid 1 \\
 \text{binary numeral}& n ::=& \epsilon \mid nd  \\
 \text{expression}& e ::=& n \mid e + e \mid e \times e
\end{array}
\]
Semantics\\
\begin{minipage}{0.4\textwidth}
\begin{align*}
 N\BR{ \epsilon } &= 0 \\
 N\BR{ n d } &= 2 N\BR{ n } + d\\[1ex]
\end{align*}
\end{minipage}
\begin{minipage}{0.6\textwidth}
  \begin{align*}
  E\BR{ n } &= N\BR{ n } \\
 E\BR{ e_1 + e_2 } &= 
    E\BR{ e_1 } + E\BR{ e_2 } \\
 E\BR{ e_1 \times e_2 } &= 
    E\BR{ e_1 } E\BR{ e_2 }
\end{align*}
\end{minipage}
\caption{Language of binary arithmetic}
\label{fig:binary-arithmetic}
\end{figure}

When writing down a semantics, one is often tempted to consider the
efficiency of a semantics, as if it were an implementation. Indeed, it
would be straightforward to transcribe the definitions of $E$ and $N$
in Figure~\ref{fig:binary-arithmetic} into your favorite programming
language and thereby obtain an interpreter. All other things being
equal, it is fine to prefer a semantics that is suggestive of an
implementation, but one should prioritize ease of understanding first.
As a result, some semantics that we study may be more declarative in
nature. This is not to say that one should not consider the efficiency
of implementations when designing a language.  Indeed, the semantics
should be co-designed with implementations to avoid accidental designs
that preclude the desired level of efficiency.  Thus, a recurring
theme of these notes will be to consider implementations of languages
alongside their semantics.



Figure~\ref{fig:interp-binary} presents an interpreter for binary
arithmetic. This interpreter, in a way reminiscent of real computers,
operates on the binary numbers directly. The auxiliary functions
$\mathit{add}$ and $\mathit{mult}$ implement the algorithms you
learned in grade school, but for binary instead of decimals.

\begin{figure}
\noindent Interpreter 
\begin{align*}
I\BR{ n } &= n \\
I\BR{ e_1 + e_2 } &=
  \mathit{add}(I\BR{ e_1 }, I\BR{ e_2 },0) \\
I\BR{ e_1 \times e_2 } &=
  \mathit{mult}(I\BR{ e_1 }, I\BR{ e_2 })
\end{align*}
%% Convert natural number to binary
%% \begin{align*}
%%  \mathit{binary}(n) &=
%%   \begin{cases}
%%      n & \text{if } n < 2 \\
%%      \mathit{binary}(n/2)\, (n \mathrel{\mathrm{mod}} 2)   &  \text{otherwise}
%%   \end{cases}
%% \end{align*}
Auxiliary Functions
\begin{align*}
  \mathit{add3}(d_1, d_2, d_3) &= 
  \text{let } n = d_1 + d_2 + d_3 \text{ in } \\
  &\quad \begin{cases}
     00 &  \text{if } n = 0 \\
     01 &  \text{if } n = 1 \\
     10 &  \text{if } n = 2 \\
     11 &  \text{if } n = 3
  \end{cases}\\
 \mathit{add}(\epsilon, \epsilon, c) &= 
   \begin{cases}
      \epsilon  & \text{if } c = 0 \\
      \epsilon 1  & \text{if } c = 1
   \end{cases}
   \\
  \mathit{add}(n_1 d_1 , n_2 d_2, c) &= 
   \mathit{add}(n_1, n_2, c')\,d_3
  \qquad \text{if } \mathit{add3}(d_1, d_2, c) = c' d_3 \\
\mathit{add}(n_1 d_1 , \epsilon, c) &= 
  \mathit{add}(n_1 d_1 , \epsilon 0, c) \\
\mathit{add}(\epsilon, n_2 d_2, c) &= 
  \mathit{add}( \epsilon 0, n_2 d_2,c) \\[1ex]
 \mathit{mult}(n_1, \epsilon) &= \epsilon \\
 \mathit{mult}(n_1, n_2 0) &= \mathit{mult}(n_1,n_2) 0 \\
 \mathit{mult}(n_1, n_2 1) &= 
  \mathit{add}(n_1, \mathit{mult}(n_1,n_2)0, 0) 
\end{align*}
\caption{Binary arithmetic interpreter.}
\label{fig:interp-binary}
\end{figure}


\begin{Exercise}
\label{ex:bit-add}
Prove that 
$N (\mathit{add3}(d_1, d_2, d_3)) = d_1 + d_2 + d_3$.
\end{Exercise}
\begin{Answer}
\begin{tabular}{l|c|c} 
    $d_1, d_2, d_3$ & $\mathit{add3}(d_1, d_2, d_3)$ 
  & $d_1 + d_2 + d_3$ \\ \hline
  $0,0,0$ & $00$ & $0$ \\
  $0,0,1$ & $01$ & $1$\\
  $0,1,0$ & $01$ & $1$ \\
  $0,1,1$ & $10$ & $2$ \\
  $1,0,0$ & $01$ & $1$ \\
  $1,0,1$ & $10$ & $2$ \\
  $1,1,0$ & $10$ & $2$\\
  $1,1,1$ & $11$ & $3$
\end{tabular} 
\end{Answer}

\begin{Exercise}
  \label{ex:add}
  Prove that $N(\mathit{add}(n_1,n_2,c)) = N(n_1) + N(n_2) + c$.
\end{Exercise}
\begin{Answer}
The proof is by induction on $\mathit{add}$.
\begin{itemize}
\item Case $N(\mathit{add}(\epsilon, \epsilon, 0)) = N\BR{\epsilon} = 0
       = N\BR{ \epsilon} + N\BR{ \epsilon} + 0$
\item Case $N(\mathit{add}(\epsilon, \epsilon, 1)) = N\BR{\epsilon 1} = 1
       = N\BR{\epsilon} + N\BR{\epsilon} + 1$
\item Case $\mathit{add}(n'_1 d_1 , n'_2  d_2, c) 
    = \mathit{add}(n_1, n_2, c')d_3$ \\
  where $\mathit{add3}(d_1, d_2, c) = c'd_3$. 
    \begin{align*}
    N(\mathit{add}(n'_1 d_1 , n'_2  d_2, c)) 
      &= N(\mathit{add}(n'_1, n'_2, c')d_3) \\
      &= N(\mathit{add}(n'_1, n'_2, c')) \times 2 + d_3 \\
      &= (N(n'_1) + N(n'_2) + c') \times 2 + d_3\\
      &= 2 N(n'_1) + 2 N(n'_2) + c' \times 2 + d_3 \\
      &= 2 N(n'_1) + 2 N(n'_2) + N\BR{c' d_3} \\
      &= 2 N(n'_1) +  d_1 + 2 N(n'_2) +  d_2 + c 
         & \text{by Ex.~\ref{ex:bit-add}}\\
      &= N \BR{n'_1 d_1} + N \BR{n'_2 d_2} + c
    \end{align*}

\item Case $\mathit{add}(n_1 d_1 , \epsilon, c) = 
  \mathit{add}(n_1 d_1 , \epsilon 0, c)$
  \begin{align*}
    N(\mathit{add}(n_1 d_1 , \epsilon, c))
    & = N\mathit{add}(n_1 d_1 , \epsilon 0, c) \\
    & = N \BR{n_1d_1} + N \BR{\epsilon 0} + c & \text{by I.H.}\\
    & = N \BR{n_1 d_1} + N\BR{\epsilon} + c
  \end{align*}

\item Case $\mathit{add}(\epsilon, n_2 d_2, c) = 
  \mathit{add}( \epsilon 0, n_2 d_2,c)$
  \begin{align*}
    N(\mathit{add}(\epsilon, n_2 d_2, c))
    &= N(\mathit{add}( \epsilon 0, n_2 d_2,c)) \\
    &= N\BR{\epsilon 0} + N\BR{n_2 d_2} + c \\
    &= N\BR{\epsilon} + N\BR{n_2 d_2} + c
  \end{align*}

\end{itemize}
\end{Answer}

\begin{Exercise}
\label{ex:mult}
Prove that $N(\mathit{mult}(n_1,n_2)) = N(n_1) N(n_2)$.
\end{Exercise}
\begin{Answer}
By induction on $\mathit{mult}$.
\begin{itemize}
\item Case $\mathit{mult}(n_1,\epsilon) = \epsilon$:\\
  \[
  N(\mathit{mult}(n_1,\epsilon)) = N(\epsilon) = 0 
  = N(n_1) N(\epsilon)
  \]

\item Case $\mathit{mult}(n_1, n'_2 0) = \mathit{mult}(n_1,n'_2) 0$:
  \begin{align*}
    N(\mathit{mult}(n_1, n'_2 0)) &= N(\mathit{mult}(n_1,n'_2) 0) \\
     & = 2 N(\mathit{mult}(n_1,n'_2)) \\
     & = 2 N(n_1) N(n'_2) & \text{by I.H.}\\
     & = N(n_1) N (n'_2 0)
  \end{align*}

\item Case $\mathit{mult}(n_1, n'_2 1) = 
  \mathit{add}(n_1, \mathit{mult}(n_1,n'_2)0)$:
  \begin{align*}
    N(\mathit{mult}(n_1, n'_2 1)) &= 
    N(\mathit{add}(n_1, \mathit{mult}(n_1,n'_2)0, 0)) \\
    &= N(n_1) + N(\mathit{mult}(n_1,n'_2)0) + 0 & \text{by Ex.~\ref{ex:add}}\\
    &= N(n_1) + 2 N(\mathit{mult}(n_1,n'_2)) \\
    &= N(n_1) + 2 N(n_1) N(n'_2) & \text{by I.H.}\\
    &= N(n_1) (2 N(n'_2) + 1) \\
    &= N(n_1) N(n'_2 1)
  \end{align*}
\end{itemize}
\end{Answer}

\begin{Exercise}
\label{binary-interp-correct}
 Prove that the interpreter is correct.
 That is
 \[
 N(I\BR{ e }) = E\BR{ e }
 \]
\end{Exercise}
\begin{Answer}
The proof is by induction on $e$.
\begin{itemize}
\item Case $e=n$: 
  $N(I\BR{ n }) = N\BR{ n } = E\BR{ n }$. 
\item Case $e = e_1 + e_2$: 
  \begin{align*}
    N(I\BR{e_1 + e_2}) &= N(\mathit{add}(I\BR{e_1}, I\BR{e_2}, 0)) \\
     & = N(I\BR{e_1}) + N(I\BR{e_2}) & \text{by Ex.~\ref{ex:add}}\\
     & = E\BR{e_1} + E\BR{e_2} & \text{by the I.H.}\\
     & = E\BR{e_1 + e_2}
  \end{align*}

\item Case $e = e_1 \times e_2$: 
  \begin{align*}
    N(I\BR{e_1 + e_2}) &= N(\mathit{mult}(I\BR{e_1}, I\BR{e_2})) \\
     & = N(I\BR{e_1}) \times N(I\BR{e_2}) & \text{by Ex.~\ref{ex:mult}}\\
     & = E\BR{e_1} \times E\BR{e_2} & \text{by the I.H.}\\
     & = E\BR{e_1 \times e_2}
  \end{align*}

\end{itemize}
\end{Answer}


%% \section{Booleans and Conditionals}

%% \[
%% \begin{array}{lrcl}
%% \text{expressions} & e & ::= & \ldots \mid \TRUE{} \mid \FALSE{} \mid \COND{e}{e}{e}
%% \end{array}
%% \]

%% local, immutable variables??

\section{An Imperative Language: IMP}

\marginnote{
\begin{tabular}{ll}
Reading: & \citet{Schmidt:1986vn} Chapter 5 \\ 
Exercises: & 5.4, 5.5 a, 5.9
\end{tabular}
}

% IMP/WHILE
% arithmetic, booleans, mutable variables, 
% commands: assignment, sequence, if, while, skip, read, write.

Figure~\ref{fig:imperative} defines a language with mutable variables,
a variant of the
IMP~\citep{Plotkin:1983aa,Winskel:1993uq,Amadio:1998fk} and
WHILE~\citep{Hoare:1969kw} languages that often appear in textbooks on
programming languages. As a bonus, we include the \texttt{while} loop,
even though \citet{Schmidt:1986vn} does not cover loops until Chapter
6. The reason for his delayed treatment of \texttt{while} loops is
that their semantics is typically expressed in terms of fixed points
of continuous functions, which takes some time to explain. However, it
turns out that the semantics of \texttt{while} loops can be defined
more simply.

To give meaning to mutable variables, we use a $\mathit{Store}$ which
is partial function from variables (identifiers) to numbers.
\[
  \mathit{Store} = \mathit{Id} \pto \mathit{Nat}
\]
We write $[x\mapsto n]s$ for removing the entry for $x$ is $s$
(if there is one) and then adding the entry $x\mapsto n$,
that is, $\{x\mapsto n\} \cup (s|_{\mathrm{dom}(s)-\{x\}})$.

\begin{figure*}[btp]
\begin{minipage}{0.5\textwidth}
\noindent Syntax
\[
\begin{array}{ll}
  \text{variables} & x \in \mathbb{X}  \\
 \text{expressions} & e ::= \ldots \mid x \\
 \text{conditions} & b  ::= \TRUE \mid \FALSE \mid e = e \mid \\
  &   \qquad \neg b \mid b \lor b \mid b \land b \\
 \text{commands}& c ::= \SKIP \mid \ASSIGN{x}{e} \mid \SEQ{c}{c} \mid\\
  &      \qquad \IF{b}{c}{c} \mid \\
  &      \qquad \WHILE{b}{c} 
\end{array}
\]
Loops
\begin{gather*}
  \inference{m_b\,s = \FALSE}
            {(s,s) \in \mathit{loop}(m_b, m_c)}
\\[1ex]
  \inference{m_b\,s_1 = \TRUE & 
             (s_1,s_2) \in m_c \\
             (s_2, s_3) \in \mathit{loop}(m_b, m_c)}
            {(s_1, s_3) \in \mathit{loop}(m_b, m_c)}
\end{gather*}
\end{minipage}
\begin{minipage}{0.5\textwidth}
Semantics 
\[
P\,c = \{ (n,s'\,\mathtt{Z}) \mid (\{\mathtt{A}{\mapsto} n\},s') \in C\,c \}
\]
\begin{align*}
C\,\SKIP &= \mathit{id} \\ %\{(s,s) \mid s \in \mathit{Store} \}\\
C(\ASSIGN{x}{e}) &= \{ (s,[x\mapsto E\,e\,s]s) \mid s {\in} \mathit{Store} \}\\
C(\SEQ{c_1}{c_2}) &=  C\,c_2 \circ C\,c_1 \\
C\left(\!\!\begin{array}{l}
  \mathtt{if}\,b\,\mathtt{then}\,c_1\\
  \mathtt{else}\,c_2
  \end{array}\!\!\right)
  &= 
 \begin{cases}
  C\,c_1 & \text{if } B\,b\,s = \TRUE \\
  C\,c_2 & \text{if } B\,b\,s = \FALSE
 \end{cases} \\
C(\WHILE{b}{c}) &= \mathit{loop}(B\,b, C\,c)
\end{align*}
\end{minipage}
\caption{An Imperative Language: IMP}
\label{fig:imperative}
\end{figure*}

The syntax of expressions is extended to include variables, so the
meaning of expressions must be parameterized on the store. The meaning
of a variable $x$ is the associated number in the store $s$.
\[
  E\,x\,s = s\,x
\]
%
A program takes a number as input and it may produce a number or it
might diverge. Traditional denotational semantics model this behavior
with a partial function $\mathit{Nat}\pto\mathit{Nat}$. Here we shall
use the alternate, but equivalent, approach of using a relation
$\mathit{Nat}\times\mathit{Nat}$. A program is a command and the
meaning of commands is given by the semantic function $C$, which maps
a command to a relation on stores, that is, to a subset of
$\mathit{Store} \times \mathit{Store}$. The meaning of a program is
given by the function $P$, which initializes the store with the input
number in variable $\mathtt{A}$.  When the program completes, the
output is obtained from variable $\mathtt{Z}$.


We define the \texttt{while} loop using an auxiliary relation named
$\mathit{loop}$, which we define inductively in
Figure~\ref{fig:imperative}. Its two parameters are for the meaning of
the condition $b$ and the body $c$ of the loop.  If the meaning of the
condition $m_b$ is $\FALSE$, then the loop is already finished so the
starting and finishing stores are the same. If the condition $m_b$ is
$\TRUE$, then the loop executes the body, relating the starting store
$s_1$ to an intermediate store $s_2$, and then the loop continues,
relating $s_2$ to the finishing store $s_3$.

%% Function update:
%% \[
%%    [x\mapsto n]s(y) =
%%  \begin{cases} n & \text{if } y = x \\
%%    s(y) & \text{if } y \neq x
%%  \end{cases} 
%% \]



We define an implementation of the imperative language, in terms of an
abstract machine, in Figure~\ref{fig:imp-impl}. The machine executes
one command at a time, similar to how a debugger such as \texttt{gdb}
can be used to view the execution of a C program one statement at a
time. Each command causes the machine to transition from one state to
the next, where a state is represented by a control component and the
store. The control component is the sequence of commands that need to
be executed, which is convenient to represent as a command of the
following form.
\[
\begin{array}{lrcl}
  \text{control} & k & ::= & \SKIP \mid \SEQ{c}{k}
\end{array}
\]
The partial function $\mathit{eval}$, also defined in
Figure~\ref{fig:imp-impl} in the main entry point for the abstract
machine.


\begin{figure*}

\hfill \fbox{$k, s \longrightarrow k', s'$}
\begin{align*}
  \SEQ{\SKIP}{k},\; s & \longrightarrow k,\; s \\
  \SEQ{(\ASSIGN{x}{e})}{k},\; s & \longrightarrow k,\; [x\mapsto E(e)(s)]s\\
  \SEQ{(\SEQ{c_1}{c_2})}{k},\; s & \longrightarrow
      \SEQ{c_1}{(\SEQ{c_2}{k})},\; s \\
  \SEQ{(\IF{b}{c_1}{c_2})}{k},\; s & \longrightarrow \SEQ{c_1}{k},\; s
     & \text{if } B\,b\,s = \TRUE \\
  \SEQ{(\IF{b}{c_1}{c_2})}{k},\; s & \longrightarrow \SEQ{c_2}{k},\; s
     & \text{if } B\,b\,s = \FALSE \\
  \SEQ{(\WHILE{b}{c})}{k},\; s & \longrightarrow
      \SEQ{c}{(\SEQ{(\WHILE{b}{c})}{k})},\; s
    & \text{if } B\,b\,s = \TRUE \\
  \SEQ{(\WHILE{b}{c})}{k},\; s & \longrightarrow k,\; s
    & \text{if } B\,b\,s = \FALSE 
\end{align*}
\[
  \mathit{eval}(c) = \{ (n,n') \mid
     (\SEQ{c}{\SKIP}), \{A\mapsto n\}
     \longrightarrow^{*} \SKIP, s'
     \text{ and } n' = s'(Z) \}
\]
\caption{Abstract Machine for IMP}
\label{fig:imp-impl}
\end{figure*}

Notation: given a relation $R$, we write $R(a)$ for the image of
$\{a\}$ under $R$. For example, if $R=\{ (0,4), (1,2), (1,3), (2,5)
\}$, then $R(1) = \{2,3\}$ and $R(0) = \{4\}$.

\begin{Exercise}
  Prove that if $k,s \longrightarrow k',s'$,
  then $C\,k\,s = C\,k'\,s'$.
\end{Exercise}
\begin{Answer}
The proof is by cases on $k,s \longrightarrow k',s'$.
\begin{itemize}
\item Case \fbox{$\SEQ{\SKIP}{k},\; s \longrightarrow k,\; s$}
  \[
  C(\SEQ{\SKIP}{k})(s) = (C(k) \circ C(\SKIP)) (s)
    = (C(k) \circ \mathit{id}) (s) = C(k)(s)
  \]

\item Case \fbox{$\SEQ{(\ASSIGN{x}{e})}{k},\; s \longrightarrow k,\; [x\mapsto E(e)(s)]s$}
\begin{align*}
 C(\SEQ{(\ASSIGN{x}{e})}{k})(s)
 &= 
 (C(k) \circ C(\ASSIGN{x}{e}))(s)\\
 &= 
 C(k)(C(\ASSIGN{x}{e})(s))\\
 &= 
 C(k)([x\mapsto E(e)(s)]s)
\end{align*}

\item Case \fbox{$\SEQ{(\SEQ{c_1}{c_2})}{k},\; s  \longrightarrow
      \SEQ{c_1}{(\SEQ{c_2}{k})},\; s$}
  \begin{align*}
    C(\SEQ{(\SEQ{c_1}{c_2})}{k})(s) 
    &=
    ( C(k) \circ (C(c_2) \circ C(c_1)) )(s) \\
    &= 
    ( (C(k) \circ C(c_2)) \circ C(c_1) )(s)\\
    &=
    C(\SEQ{c_1}{(\SEQ{c_2}{k})})(s)
  \end{align*}

\item Case \fbox{$\SEQ{(\IF{b}{c_1}{c_2})}{k},\; s 
        \longrightarrow \SEQ{c_1}{k},\; s$ and $B\,b\,s = \TRUE$}
  \begin{align*}
  C(\SEQ{(\IF{b}{c_1}{c_2})}{k})(s)
  &= (C(k) \circ C(\IF{b}{c_1}{c_2}))(s)\\
  &= (C(k) \circ C(c_1))(s)\\
  &= C(\SEQ{c_1}{k})(s)
  \end{align*}

\item Case \fbox{$\SEQ{(\IF{b}{c_1}{c_2})}{k},\; s 
        \longrightarrow \SEQ{c_2}{k},\; s$ and $B\,b\,s = \FALSE$}
  \begin{align*}
  C(\SEQ{(\IF{b}{c_1}{c_2})}{k})(s)
  &= (C(k) \circ C(\IF{b}{c_1}{c_2}))(s)\\
  &= (C(k) \circ C(c_2))(s)\\
  &= C(\SEQ{c_2}{k})(s)
  \end{align*}

\item Case \fbox{$\SEQ{(\WHILE{b}{c})}{k},\; s \longrightarrow
      \SEQ{c}{(\SEQ{(\WHILE{b}{c})}{k})},\; s$ and $B\,b\,s = \TRUE$}
  \begin{align*}
    C(\SEQ{(\WHILE{b}{c})}{k})(s)
    &= (C(k) \circ C(\WHILE{b}{c})) (s)\\
    &= (C(k) \circ (C(\WHILE{b}{c}) \circ C(c))) (s)\\
    &= ((C(k) \circ C(\WHILE{b}{c})) \circ C(c)) (s)\\
    &= C(\SEQ{c}{(\SEQ{(\WHILE{b}{c})}{k})})(s)
  \end{align*}

\item Case \fbox{$\SEQ{(\WHILE{b}{c})}{k},\; s \longrightarrow k,\; s$ 
  and $B\,b\,s = \FALSE$}
  \begin{align*}
    C(\SEQ{(\WHILE{b}{c})}{k})(s) 
    &= (C(k) \circ C(\WHILE{b}{c}))(s)\\
    &= (C(k) \circ \mathit{id})(s)\\
    &= C(k)(s)
  \end{align*}
\end{itemize}
\end{Answer}

\begin{Exercise}
  Prove that $P(c) = \mathit{eval}(c)$.
\end{Exercise}
\begin{Answer}
TODO
\end{Answer}


\section{Recursive Definitions via Least Fixed Points}

\marginnote{
\begin{tabular}{ll}
Reading: & \citet{Schmidt:1986vn} Chapter 6 \\
Exercises: & 6.2 a, 6.6, 8
\end{tabular}
}

We shall revisit the semantics of the imperative language, this time
taking the traditional but more complex approach of defining
\texttt{while} with a recursive equation and using least fixed points
to solve it. Recall that the meaning of a command is a partial
function from stores to stores, or more precisely,
\[
C\,c : \mathit{Store} \pto \mathit{Store}
\]
The meaning of a loop $(\WHILE{b}{c})$ is a solution to the
equation
\begin{equation}
\label{eq:w}
w = \LAM{s}\mathit{if}\, B\,b\,s \,\mathit{then}\, w(C\,c\,s)\,\mathit{else}\,s
\end{equation}
In general, one can write down recursive equations that do not have
solutions, so how do we know whether this one does?  When is there a
unique solution? The theory of least fixed points provides answers to
these questions.

\begin{definition}
A \textbf{\emph{fixed point}} of a function is an element that gets
mapped to itself, i.e., $x = f(x)$.
\end{definition}

In this case, the element that we're interested in is $w$, which is
itself a function, so our $f$ will be higher-order function. We
reformulate Equation~\ref{eq:w} as a fixed point equation by
abstracting away the recursion into a parameter $r$.
\begin{equation} \label{eq:while}
  w = F_{b,c}(w)
  \qquad
  \text{where }
  F_{b,c}\,r\,s = \mathit{if}\, B\,b\,s \,\mathit{then}\, r(C\,c\,s)\,\mathit{else}\,s
\end{equation}

There are quite a few theorems in the literature that guarantee the
existence of fixed points, but with different assumptions about the
function and its domain. For our purposes, the CPO Fixed-Point Theorem
will do the job. The idea of this theorem is to construct an infinite
sequence that provides increasingly better approximations of the least
fixed point. The union of this sequence will turn out to be the least
fixed point.

The CPO Fixed-Point Theorem is quite general; it is stated in terms of
a function $F$ over partially ordered sets with a few mild conditions.
The ordering captures the notion of approximation, that is, we write
$x \sqsubseteq y$ if $x$ approximates $y$.

\begin{definition}
  A \textbf{\emph{partially ordered set (poset)}} is a pair
  $(L,\sqsubseteq)$ that consists of a set $L$ and a partial order
  $\sqsubseteq$ on $L$.
\end{definition}

\begin{marginfigure}
\centering\large
\xymatrix@=10pt{
 & \left\{
   \substack{2\mapsto 4, \\ 3\mapsto 9, \\4\mapsto 16}
  \right\} \ar@{-}[dl]\ar@{-}[d] \ar@{-}[dr]& \\
 \left\{ \substack{2\mapsto 4,\\ 3\mapsto 9} \right\} \ar@{-}[d]\ar@{-}[dr] 
  & 
 \left\{ \substack{2\mapsto 4,\\ 4\mapsto 16} \right\} \ar@{-}[dl]\ar@{-}[dr] 
  &
\left\{ \substack{3\mapsto 9,\\ 4 \mapsto 16} \right\} \ar@{-}[d]\ar@{-}[dl]
  \\
\{\substack{2\mapsto 4}\} \ar@{-}[dr] &  \{\substack{3\mapsto 9}\} \ar@{-}[d]& \{\substack{4\mapsto 16}\} \ar@{-}[dl]\\
  & \emptyset
}
\caption{A poset of partial functions.}
\label{fig:poset-of-partial-functions}
\end{marginfigure}
%
For example, consider the poset
$(\mathbb{N}{\pto}\mathbb{N}, \subseteq)$ of partial
functions on natural numbers. Some partial function $f$ is a better
approximation than another partial function $g$ if it is defined on
more inputs, that is, if the graph of $f$ is a subset of the graph of
$g$.  Two partial functions are incomparable if neither is a subset of
the other.

The sequence of approximations will start with the worst
approximation, a bottom element, written $\bot$, that contains no
information. (For example, $\emptyset$ is the $\bot$ for the poset of
partial functions.)
%% \begin{wrapfigure}{r}{0.33\textwidth}
%% \small \textbf{Example} For \texttt{while}, each $g_i$ corresponds to
%% iterating the loop up to $i$ times.
%% \end{wrapfigure}
The sequence proceeds to by applying $F$ over and over again, that is,
\[
  \bot \quad F(\bot)\quad F(F(\bot))\quad F(F(F(\bot)))\quad \cdots\quad F^i(\bot)\quad \cdots
\]

But how do we know that this sequence will produce increasingly better
approximations?  How do we know that
\[
F^i(\bot) \sqsubseteq F^{i+1}(\bot)
\]
We can ensure this by requiring the output of $F$ to improve when the
input improves, that is, require $F$ to be monotonic.

\begin{definition}
  Given two partial orders $(A,\sqsubseteq)$ and
  $(B,\sqsubseteq)$, $F : A {\to} B$ is \textbf{\emph{monotonic}} iff
  for any $x,y\in A$, $x \sqsubseteq y$ implies $F(x) \sqsubseteq F(x)$.
\end{definition}

\begin{proposition}
  The functional $F_{b,c}$ for \texttt{while} loops \eqref{eq:while}
  is monotonic.
\end{proposition}
\begin{proof}
  Let $f, g : \mathit{Store} \pto \mathit{Store}$ such that
  $f \subseteq g$. We need to show that $F_{b,c}(f) \subseteq
  F_{b,c}(g)$. 
  Let $s$ be an arbitrary state. Suppose $B\,b\,s = \TRUE$.
  \[
  F_{b,c}\,f\,s = f(C\,c\,s) \subseteq g(C\,c\,s) = g(C\,c\,s) = F_{b,c}\,g\,s 
  \]
  So we have $F_{b,c}(f) \subseteq F_{b,c}(g)$.
  Next suppose $B\,b\,s = \FALSE$.
  \[
  F_{b,c}\,f\,s = s
               = F_{b,c}\,g\,s 
  \]
  So again we have $F_{b,c}(f) \subseteq F_{b,c}(g)$.  Having
  completed both cases, we conclude that $F_{b,c}(f) \subseteq
  F_{b,c}(g)$.
\end{proof}


\begin{marginfigure}
\[
  \bot \sqsubseteq F(\bot) \sqsubseteq F^2(\bot) 
    \sqsubseteq F^3(\bot) \sqsubseteq \cdots
\]
\caption{Ascending chain of $F$.}
\label{fig:kleene-chain}
\end{marginfigure}
%
We have $\bot \sqsubseteq F(\bot)$ because $\bot$ is less or equal to
everything. Then we apply monotonicity to obtain $F(\bot) \sqsubseteq
F(F(\bot))$.  Continuing in this way we obtain the sequence of
increasingly better approximations in Figure~\ref{fig:kleene-chain}.
If at some point the approximation stops improving, but instead $F$
produces an element that is merely equal to the last one, then we have
found a fixed point. However, because we are interested in elements
that are partial functions, which are infinite, the sequences of
approximations will also be infinite. So we'll need some other way to
go from the sequences of approximations to the actual fixed point.

The solution is to take the union of all the approximations. The
analogue of union for an arbitrary partial order is least upper bound.

\begin{marginfigure}
$\bigsqcup \left\{\!\! \begin{array}{l}
              \{ 2 \mapsto 4, 3 \mapsto 9 \}, \\
              \{ 3 \mapsto 9, 4 \mapsto 16 \} 
                 \end{array}\!\! \right\}
= \left\{\!\! \begin{array}{l}
        2 \mapsto 4, \\ 3 \mapsto 9, \\ 4 \mapsto 16 
    \end{array}\!\! \right\}$
\caption{The lub of partial functions.}
\end{marginfigure}

\begin{definition}
Given a subset $S$ of a partial order $(L,\sqsubseteq)$, an
\textbf{\emph{upper bound}} of $S$ is an element $y$ such that for all
$x \in S$ we have $x \sqsubseteq y$.  The \textbf{\emph{least upper
    bound (lub)}} of $S$, written $\bigsqcup S$, is the least of all
the upper bounds of $S$, that is, given any upper bound $z$ of $S$, we
have $\bigsqcup S \sqsubseteq z$.
\end{definition}

In arbitrary posets, a least upper bound may not exist for an
arbitrary subset. In particular, for the poset
$(\mathbb{N}{\pto}\mathbb{N}, \subseteq)$, two partial
functions do not have a lub if they are inconsistent, that is, if they
map the same input to different outputs, such as $\{3\mapsto 8\}$ and
$\{3 \mapsto 9\}$. However, the CPO Fixed-Point Theorem will only need to
consider totally ordered subsets, i.e., chains, and all the elements
in a chain are consistent.

\begin{definition}
  A \textbf{\emph{chain}} is a totally ordered subset of a poset.  A
  \textbf{\emph{chain-complete partial order (cpo)}} has a least upper
  bound for every chain.
\end{definition}


%% \begin{proof}
%%   Let $S$ be a directed subset of $A \to B_\bot$.
%%   The union of the elements of $S$ is the least upper bound of $S$.
%%   Suppose $h \in S$. Then $h \subseteq \bigcup S$, so $\bigcup S$ is an upper bound of $S$.
%%   Let $f$ be an arbitrary upper bound of $S$.
%%   We need to show that $\bigcup S \subseteq f$.
%%   Suppose $(x,y) \in \bigcup S$.
%%   Then $(x,y) \in g$ for some $g \in S$.
%%   So $(x,y) \in f$ because $g \subseteq f$.
%%   Therefore $\botcup S$ is the least of all the upper bounds of $S$.
%% \end{proof}

%% I like dcpos because it seems quite natural to say that lubs should
%% exist for sets of consistent elements. On the other hand, the
%% chain-based definition gives just what is needed for Kleene's Fixed
%% Point Theorem.
%% The two schools of thought yield definitions that are
%% equivalent~\citep{Mitchell:1996nn} (page 312).

\begin{proposition}
  The poset of partial functions $(A\pto B,\subseteq)$ is a
  cpo.
\end{proposition}
\begin{proof}
  Let $S$ be a chain in $(A\pto B,\subseteq)$.  We claim
  that the lub of $S$ is just the union of all the elements of $S$,
  that is $\bigcup S$. Recall that $\forall x y, (x,y) \in \bigcup S$
  iff $\exists f \in S, (x,y) \in f$.
%
  We first need to show that $\bigcup S$ is an upper bound of $S$.
  Suppose $f \in S$. We need to show that $f \subseteq \bigcup S$.
  Consider $(x,y) \in f$. Then $(x,y) \in \bigcup S$. So indeed,
  $\bigcup S$ is an upper bound of $S$.  Second, consider another
  upper bound $g$ of $S$.  We need to show that $\bigcup S \subseteq
  g$. Suppose $(x,y) \in \bigcup S$.  Then $(x,y) \in h$ for some $h
  \in S$. Because $g$ is an upper bounf of $S$, we have $h \subseteq
  g$ and therefore $(x,y) \in g$. So we conclude that $\bigcup S
  \subseteq g$.
\end{proof}

The last ingredient required in the proof of the fixed point theorem
is that the output of $F$ should only depend on a finite amount of
information from the input, that is, it should be continuous. For
example, if the input to $F$ is itself a function $g$, $F$ should only
need to apply $g$ to a finite number of different values.  This
requirement is at the core of what it means for a function to be
computable~\citep{Gunter:1990aa}.  So applying $F$ to the lub of a
directed set $X$ (an infinite thing) should be the same as taking the
lub of the set obtained by mapping $F$ over the elements of $X$
(finite things).

\begin{definition}
  A monotonic function $F : A {\to} B$ on a cpo is
  \textbf{\emph{continuous}} iff for all chains $X$ of $A$
  \[
  F(\bigsqcup X) = \bigsqcup \{ F(x) \mid x \in X \}
  \]
\end{definition}

\begin{proposition}
  The functional $F_{b,c}$ for \texttt{while} loops \eqref{eq:while}
  is continuous.
\end{proposition}
\begin{proof}
  Let $S$ be a chain in $(A\pto B,\subseteq)$. 
  We need to show that
  \[
  F_{b,c}(\bigcup S) = \bigcup \{ F_{b,c}(f) \mid f \in S\}
  \]
  We shall prove this equality by showing that each graph is a subset
  of the other.  We assume $(s,s'') \in F_{b,c}\,(\bigcup S)$.
  Suppose $B\,b\,s = \TRUE$.  Then $(s,s'') \in (\bigcup S) \circ (C
  \, c)$, so $(s,s') \in C\,c$ and $(s',s'') \in g$ for some $s'$ and
  $g \in S$. So $(s,s'') \in g \circ (C\, c)$.  Therefore $(s,s'') \in
  \bigcup \{ F_{b,c}(f) \mid f \in S \}$.  So we have shown that
  $F_{b,c}\,(\bigcup S) \subseteq \bigcup \{ F_{b,c}(f) \mid f \in S
  \}$. Next suppose $B\,b\,s = \FALSE$.  Then
  $
  F_{b,c}(\bigcup S) = \mathit{id} = \bigcup \{ F_{b,c}(f) \mid f \in S\}
  $.

  For the other direction, we assume $(s,s'') \in \bigcup \{
  F_{b,c}(f) \mid f \in S\}$. So $(s,s'') \in F_{b,c}(g)$ for some $g
  \in S$.  Suppose $B\,b\,s = \TRUE$.  Then $(s,s') \in C\,c$ and
  $(s',s'') \in g$ for some $s'$. So $(s',s'') \in \bigcup S$ and
  therefore $(s,s'') \in F_{b,c}(\bigcup S)$.  So we have shown that
  $\bigcup \{ F_{b,c}(f) \mid f \in S \} \subseteq F_{b,c}\,(\bigcup
  S)$. Next suppose $B\,b\,s = \FALSE$. Then again we have both graphs
  equal to the identity relation.
\end{proof}

We now state the fixed point theorem for cpos.

\begin{theorem}[CPO Fixed-Point Theorem]\label{thm:fixed-point}
Suppose $(L,\sqsubseteq)$ is a cpo and let $F:L\to L$ be a continuous
function. Then $F$ has a least fixed point, written $\mathrm{fix}\,F$,
which is the least upper bound of the ascending chain of $F$:
\[
  \mathrm{fix}\,F = \bigsqcup \{ F^n(\bot) \mid n \in \mathit{Nat} \}
\]
\end{theorem}
\begin{proof}
Note that $\bot$ is an element of $L$ because it is the lub of the
empty chain.  We first prove that $\mathrm{fix}\,F$ is a fixed point
of $F$.
\begin{align*}
  F(\mathrm{fix}\,F) &= F(\bigsqcup \{ F^n(\bot) \mid n \in \NAT \})\\
  &= \bigsqcup\{ F(F^n(\bot)) \mid n \in \NAT \} & \text{by continuity}\\
  &= \bigsqcup\{ F^{n+1}(\bot)) \mid n \in \NAT \} \\
  %% &= \bigsqcup \{ F^1(\bot), F^2(\bot), F^3(\bot), \ldots\} \\
  %% &= \bigsqcup \{ F^0(\bot), F^1(\bot), F^2(\bot), F^3(\bot), \ldots\} \\
  &= \bigsqcup\{ F^n(\bot)) \mid n \in \NAT \} 
  & \text{because } F^0(\bot) = \bot \sqsubseteq F^1(\bot)  \\
  &= \mathrm{fix}\,F
\end{align*}
Next we prove that $\mathrm{fix}\,F$ is the least of the fixed points
of $F$. Suppose $e$ is an arbitrary fixed point of $F$. By the
monotonicity of $F$ we have $F^i(\bot) \sqsubseteq F^i(e)$ for all
$i$.  And because $e$ is a fixed point, we also have $F^i(e) = e$, so
$e$ is an upper bound of the ascending chain, and therefore
$\mathrm{fix}\,F \sqsubseteq e$.
\end{proof}


Returning to the semantics of the \texttt{while} loop, we give the
least fixed-point semantics of an imperative language in
Figure~\ref{fig:imperative-fixed-point}. We have already seen that the
poset of partitial functions is a cpo and that $F_{b,c}$ is
continuous, so $\mathrm{fix}\,(F_{b,c})$ is well defined and is the
least fixed-point of $F_{b,c}$. Thus, we can define the meaning
of the \texttt{while} loop as follows.
\[
  C(\WHILE{b}{c})\,s = \mathrm{fix}(F_{b,c})\,s
\]

\begin{marginfigure}
\[
P'\,c\,n = (C'\,c\,\{A{\mapsto} n]\})\,Z 
\]
\begin{align*}
C'\,\SKIP\,s &= s \\ 
C'(\ASSIGN{x}{e})\,s &= [x\mapsto E\,e\,s]s\\
C'(\SEQ{c_1}{c_2})\,s &=  C'\,c_2(C'\,c_1\,s) \\
C'\left(\!\!\begin{array}{l}
  \mathtt{if}\,b\,\mathtt{then}\,c_1\\
  \mathtt{else}\,c_2
  \end{array}\!\!\right)\,s
  &= 
 \begin{cases}
  C'\,c_1\,s & \text{if } B\,b\,s = \TRUE \\
  C'\,c_2\,s & \text{if } B\,b\,s = \FALSE
 \end{cases} \\
C'(\WHILE{b}{c})\,s &= \mathrm{fix}(F_{b,c})\,s
\end{align*}
\caption{Least Fixed-Point Semantics of an Imperative Language}
\label{fig:imperative-fixed-point}
\end{marginfigure}


\begin{Exercise}
  Prove that the semantics in Figure~\ref{fig:imperative} is
  equivalent to the least fixed-point semantics in
  Figure~\ref{fig:imperative-fixed-point}, i.e., $(n,n') \in P\,c$ iff
  $P\,c\,n = n'$.
\end{Exercise}
\begin{Answer}
TODO
\end{Answer}

In the literature there are two schools of thought regarding how to
define complete partial orders. There is the one presented above, that
requires lubs to exists for all
chains~\citep{Plotkin:1983aa,Schmidt:1986vn,Winskel:1993uq}.  The
other requires that the poset be directed complete, that is, lubs
exists for all directed
sets~\citep{Gunter:1990aa,Mitchell:1996nn,Amadio:1998fk}.  The two
schools of thought are equivalent, i.e., a poset $P$ with a least
element is directed-complete iff every chain in $P$ has a
lub~\citep{Davey:2002fj} (Theorem 8.11).


\section{A Functional Language: the $\lambda$-calculus}
\label{sec:lambda}

\marginnote{
\begin{tabular}{ll}
Reading: & \citet{Siek:2017ab} 
\end{tabular}
}

We now turn to the prototypical functional language, the
$\lambda$-calculus. The main feature of this language is a function,
created by lambda abstractions $\LAM{x}e$. The application expression
$(e_1\,e_2)$ calls the function produced by $e_1$ with the argument
produced by $e_2$. We study here the call-by-value (CBV) version of
the $\lambda$-calculus.  As a secondary feature, we include binary
arithmetic (Section~\ref{sec:binary-arithmetic}).

In the $\lambda$-calculus, functions are first-class entities, so they
can be passed to other functions and returned from them. This
introduces a difficulty in trying to give a semantics in terms of
regular mathematical sets and functions. It seems that we need a set
$\mathbb{D}$ that solves the following equation.
\[
   \mathbb{D} = \mathbb{N} + (\mathbb{D} \pto \mathbb{D})
\]
But such a set cannot exist because the size of $\mathbb{D} \pto
\mathbb{D}$ is necessarily larger than $\mathbb{D}$!

There are several ways around this problem. One approach is to
consider only continuous functions, which cuts down the size enough to
make it possible to solve the equation. The first model of the
$\lambda$-calculus, $D_\infty$ of \citet{Scott:1970dp}, takes this
approach. We shall study $D_\infty$ in Section~\ref{sec:D-infinity}.

Another approach does not require solving the above equation, but
recognizes that when passing a function to another function, one
doesn't need to pass the entirety of the function (which is infinite),
but one can instead pass a finite subset of the function's graph.  The
trouble of deciding which finite subset to pass can be sidestepped by
trying all possible subsets. This would of course be prohibitive if
our current goal was to implement the $\lambda$-calculus, but this
``inefficiency'' does not pose a problem for a semantics, a
\emph{specification}, of the $\lambda$-calculus. The various semantics
that take this approach are called \emph{graph models}. This first
such model was $T^{*}_C$ of \citet{Plotkin:1972aa}. Then came
$\mathcal{P}\omega$ of \citet{Scott:1976lq} and the simpler $D_A$ of
\citet{Engeler:1981aa}. We shall study these three models in
Section~\ref{sec:graph-models}. But first, we study perhaps the most
straightforward of the graph models, the recent one by yours
truly~\citep{Siek:2017ab}.

\begin{figure*}
\begin{minipage}{0.5\textwidth}
\noindent Syntax
\[
\begin{array}{lrl}
 \text{vars.} & x \in \mathbb{X}\\
 \text{expr.}& e \in\mathbb{E} ::=& n \mid e + e \mid e \times e \mid x \mid \LAM{x} e \mid (e \, e)
\end{array}
\]
Domain
\[
\begin{array}{lrl}
               & n \in \mathbb{N} \\
  \text{elts.} & d \in \mathbb{D} ::= &n \mid \{ (d_1,d'_1), \ldots, (d_n,d'_n) \} \\
  \text{env.} & \rho \in  \mathbb{X} \pto \mathbb{D} 
\end{array}
\]
Ordering
\[
   n \sqsubseteq n \qquad 
   \inference{t \subseteq t'}{t \sqsubseteq t'}
\]
Abstraction and Application
\begin{align*}
  \uplambda^S & : (\mathbb{D} \to \mathcal{P}(\mathbb{D})) \to \mathcal{P}(\mathbb{D}) \\
  \uplambda^S f &\defeq
      \{ t \mid \forall (d,d') \in t, d' \in f\, d \} \\[1ex]
  D^{*}_1 \cdot D^{*}_2 & \defeq
    \left\{ d' \middle|
    \begin{array}{l}
      \exists t d d_2, t \in D^{*}_1,  d_2 \in D^{*}_2, \\
      (d,d') \in t, \text{ and } d \sqsubseteq d_2
    \end{array}
    \right\}
\end{align*}
\end{minipage}
\begin{minipage}{0.5\textwidth}
Semantics
\begin{align*}
  P\,e &\defeq \{ d \mid \exists d'.\, d \sqsubseteq d' 
               \text{ and } d' \in E\,e\,\emptyset\} \\[1ex]
  E & : \mathbb{E} \to (\mathbb{X}\to \mathbb{D}) \to \mathcal{P}(\mathbb{D})\\
  E\, n \, \rho & = \{ N(n) \} \\
  E\, (e_1 + e_2) \, \rho & = \{ n_1 + n_2 \mid n_1 \in E\,e_1\,\rho, 
                                    n_2 \in E\,e_2\,\rho \} \\
  E\, (e_1 \times e_2) \, \rho & = \{ n_1 n_2 \mid n_1 \in E\,e_1\,\rho, 
                                    n_2 \in E\,e_2\,\rho \} \\
  E\, x \,\rho &= \{ \rho\,x \} \\
  E\, (\LAM{x}e)\,\rho &= \uplambda^S (\uplambda d. E\,e\,[x\mapsto d]\rho) \\
  E\, (e_1\,e_2)\,\rho &= (E\,e_1\,\rho) \cdot (E\,e_2\,\rho)
\end{align*}
\end{minipage}
\caption{$\lambda$-calculus}
\label{fig:lambda}
\end{figure*}

Figure~\ref{fig:lambda} defines the syntax and semantics of a CBV
$\lambda$-calculus. To enable trying all possible finite subsets of a
function's graph, the semantics is non-deterministic. That is, the
semantic function $E$ maps programs to sets of elements, instead of a
single element. In the case when an expression produces a natural
number, the set is just a singleton of that number. However, when the
expression produces a function, the set contains all finite
approximations of the function.

The domain $\mathbb{D}$ consists of just the natural numbers and
finite graphs (association tables) of functions.  $\mathcal{D}$ is
defined inductively, or equivalently, it is the least solution of
\[
   \mathbb{D} = \mathbb{N} + \mathcal{P}_f(\mathbb{D} \times \mathbb{D})
\]
Let $t$ range over the finite graphs, that is, $t \in
\mathcal{P}_f(\mathbb{D} \times \mathbb{D})$, $D$ range over finite
sets of elements from $\mathbb{D}$, and $D^{*}$ range over possibly
infinite sets of elements from $\mathbb{D}$.

The semantics in Figure~\ref{fig:lambda} says the meaning of an
abstraction $(\LAM{x}e)$ is the set of all tables $t$ such that each
input-output entry $(d,d')$ makes sense for the $\lambda$. That is,
$d' \in E\,e\,[x\mapsto d]\rho$. While this is straightforward to
express directly, we make use of an auxilliary function $\uplambda^S$,
also defined in Figure~\ref{fig:lambda}, which is useful in connecting
this semantics to the notions of $\lambda$-models and reflexive cpos
from the literature. 
[I'm not so sure about this anymore -Jeremy]
So far we have discussed two of the three
lambda's on the line that gives meaning to abstractions: $\lambda$ is
the syntactic entity in $\mathbb{E}$ and $\uplambda^S$ is the helper
function defined in ~\ref{fig:lambda}.  The third lambda, the
$\uplambda$ in $(\uplambda d. E\,e\,[x\mapsto d]\rho)$, is the
notation we use for defining a good-old mathematical function.  (The
author thinks in HOL, so he really has the lambda of
HOL~\cite{Nipkow:2002jl} in mind.)

The semantics of application is defined in terms of the auxilliary
application operator $D^{*}_1 \cdot D^{*}_2$, which applies all the
tables in $D^{*}_1$ to all the elements (arguments) in $D^{*}_2$. A
naive definition of the application operator is just table lookup in a
non-deterministic setting.
\begin{align*}
  D^{*}_1 \cdot D^{*}_2 &= 
  \{ d' \mid
  \exists t d.\, t \in D^{*}_1, d \in D^{*}_2, 
    (d,d') \in t
  \}
  & \text{(naive)}
\end{align*}
The idea is to collect up all the results $d'$ that come from matching
up an argument $d \in D^{*}_2$ with an entry $(d,d')$ from a table $t \in
D^{*}_1$. The problem with this naive version is that it prohibits self
application, which is an important part of the $\lambda$-calculus.
For example, it is necessary to define the $Y$ combinator and thereby
express general recursion. The problem is that in self application,
one would need a table to be inside itself, that is, $(t,d') \in t$.
But that can't happen because the domain $\mathbb{D}$ is inductively
defined. The solution is to allow the argument to be a larger table
than the input $d$ of the table entry. We define the approximation
ordering $\sqsubseteq$ in Figure~\ref{fig:lambda}.  So then, for
argument $d_2 \in D^{*}_2$, we require $d \sqsubseteq d_2$ instead of $d =
d_2$.

We turn to an implementation of the $\lambda$-calculus, the
interpreter in Figure~\ref{fig:interp-lambda}. The interpreter is
based on the notion of a closure, which pairs a $\lambda$ abstraction
with its environment to ensure that the free variables get their
definitions from the lexical scope.

\begin{figure}
\[
\begin{array}{lrl}
 \text{values} & v \in \mathbb{V} ::= & n \mid \langle \LAM{x}e, \varrho \rangle \\
 \text{env.} & \varrho \in \mathbb{X} \pto \mathbb{V} 
\end{array}
\]
\begin{align*}
  I\,n\,\varrho &= n \\
  I\,(e_1 + e_2)\,\varrho &= \mathit{add}(I\,e_1\,\varrho, I\,e_2\,\varrho, 0) \\
  I\,(e_1 \times e_2)\,\varrho &= \mathit{mult}(I\,e_1\,\varrho, I\,e_2\,\varrho) \\
  I\,(\LAM{x}e)\,\varrho &= \langle \LAM{x}e, \varrho \rangle \\
  I\,(e_1\,e_2)\,\varrho &=  I\,e\,[x\mapsto I\,e_2\,\varrho]\varrho'\\
      & \quad \text{if } I\,e_1\,\varrho = \langle \LAM{x}e,\varrho' \rangle
\end{align*}

\caption{Interpreter for the $\lambda$-calculus}
\label{fig:interp-lambda}
\end{figure}

We prove the correctness of the interpreter in two steps.  First we
show that if the semantics says the result of a program should be an
integer $i$, then the interpreter produces a binary numeral $n$ such
that $N(n) = i$. The second part is to show that if the interpreter
produces an answer $n$, then the semantics agrees that $N(n)$ should
be the answer.

For the first part, we need to relate denotations (elements $d$) to
the values $v$ used by the interpreter.
\begin{align*}
  \mathcal{G}(i) &= \{ n \mid N(n) = i \} \\
  \mathcal{G}(t) &= \left\{ \langle \LAM{x} e, \varrho \rangle \middle|
     \begin{array}{l}
       \forall (d,d') \in t, v \in \mathcal{G}(d),\\
       \exists v'. I\,e\,[x\mapsto v]\varrho = v'
               \text{ and } v'\in\mathcal{G}(d')
     \end{array} \right\}
\end{align*}
Similarly, we related semantic environments to the
interpreter's environments.
\[
  \inference{}{\mathcal{G}(\emptyset,\emptyset)}
  \qquad
  \inference{v \in \mathcal{G}(d) & \mathcal{G}(\rho,\varrho)}
            {\mathcal{G}([x\mapsto d] \rho, [x\mapsto v] \varrho}
\]

\noindent Leading up to the first theorem, we establish the following lemmas.

\begin{lemma}[$\mathcal{G}$ is downward closed]
\label{lem:sub-good}
If $v \in \mathcal{G}(d)$ and $d' \sqsubseteq d$, then $v \in \mathcal{G}(d')$.
\end{lemma}

\begin{lemma}
\label{lem:lookup-good}
  If $\mathcal{G}(\rho,\varrho)$,
  then $\varrho(x) \in \mathcal{G}(\rho(x))$
\end{lemma}

\begin{lemma}
  If $d \in E\,e\,\rho$ and $\mathcal{G}(\rho,\varrho)$, then
  $I\,e\,\varrho = v$, $v \in \mathcal{G}(d)$ for some $v$.
\end{lemma}

\begin{theorem}[Adequacy]
If $E\,e\,\emptyset = E\,i\,\emptyset$, then $I\,e\,\emptyset = n$
and $N(n) = i$.
\end{theorem}

UNDER CONSTRUCTION







%% \begin{theorem}
%%   $\mathcal{P}(\mathbb{D})$ is a reflexive cpo by
%%   choosing $F=\uplambda x.\uplambda y. x \cdot y$ and
%%   $G=\uplambda^S$.

%% \end{theorem}
%% \begin{proof}
%%   TODO: prove that $F$ and $G$ are continuous and that
%%   $(F \circ G)\, f = f$.

%% \begin{align*}
%%   (F \circ G)\, f &= \uplambda y. (\uplambda^S f) \cdot y \\
%%      &= \uplambda y. \{ d' \mid \exists t d d_2,
%%             t \in (\uplambda^S f), d_2 \in y, (d,d') \in t, d \sqsubseteq d_2 \}\\
%%      &= \uplambda y. \{ d' \mid \exists t d d_2,
%%             (\forall (d_0,d'_0) \in t, d'_0 \in f d_0),
%%             d_2 \in y, (d,d') \in t, d \sqsubseteq d_2 \}\\
%%      &= \uplambda y. \{ d' \mid \exists d d_2,
%%             d_2 \in y, d' \in f\, d, d \sqsubseteq d_2 \}\\
%%      & \vdots \\
%%      &= \uplambda y. \{ d' \mid ... \}\\
%%      &= f
%% \end{align*}

%% \end{proof}


%===============================================================================
%===============================================================================
\section{Graph models of $\lambda$-calculus $(T^{*}_C$, $D_A$, and $\mathcal{P}(\omega))$ }
\label{sec:graph-models}



%===============================================================================
\subsection{The $T^{*}_C$ model of \citet{Plotkin:1972aa}}
\label{sec:t-c}


\newcommand{\EP}[0]{E_P}
\newcommand{\PSEM}[1]{\EP\,#1\,}

  Let $D$ range over finite sets of elements from $T_C$
  and $D^{*}$ range over possibly infinite sets.
  Figure~\ref{fig:t-c} defines the semantics.
  

\begin{figure}[tbp]
\begin{align*}
  T_C &= C + \mathcal{P}_f(T_C) \times \mathcal{P}_f(T_C) \\
  T^{*}_C &= \mathcal{P}(T_C) \\
  \uplambda^P &: [T^{*}_C \to T^{*}_C] \to T^{*}_C \\
  \uplambda^P f &= \{ (D,D') \mid D' \subseteq f\,D \} \\
  - \cdot_P - &: T^{*}_C \to T^{*}_C \to T^{*}_C \\
  D^{*}_1 \cdot_P D^{*}_2 &\defeq 
     \bigcup\{ D'\mid \exists D, (D,D') \in D^{*}_1, D \subseteq D^{*}_2\}\\[1ex]
%% \PSEM{ n }\rho &= \{ n \} \\
%% \PSEM{ e_1 \oplus e_2 }\rho &= \{  n_1 \oplus n_2 \mid 
%%    n_1 \in \PSEM{ e_1 }\rho \land n_2 \in \PSEM{ e_2 }\rho \} \\
\EP&: \mathbb{E} \to (\mathbb{X}\to T^{*}_C) \to T^{*}_C\\
\PSEM{ x }\rho &= \rho(x) \\
%\PSEM{ (\LAM{x} e) }\rho &= 
%  \{ (D,D') \mid  D' \subseteq \PSEM{ e }[x\mapsto D]\rho \} \\
\PSEM{ (\LAM{x} e) }\rho &= \uplambda^P (\uplambda D^{*}.\, \PSEM{e}[x\mapsto D^{*}]\rho) \\
\PSEM{ (e_1\,e_2) }\rho &= (\PSEM{ e_1 }\rho) \cdot_P (\PSEM{ e_2 }\rho)
\end{align*}
\caption{The $T^{*}_C$ model}
\label{fig:t-c}
\end{figure}


%===============================================================================
\subsection{The $D_A$ model of \citet{Engeler:1981aa}}
\label{sec:d-a}

\newcommand{\EE}[0]{E_E}
\newcommand{\ESEM}[1]{\EE\, #1 \,}


  Let $d$ range over elements of $B_A$.
  Let $D$ range over finite sets of elements from $B_A$ and $D^{*}$
  range over possibly infinite sets.  Figure~\ref{fig:d-a} gives the
  semantics, following the presentation of
  \citet{barendregt84:_lambda_calculus} (Section 5.4).
  

\begin{figure}[tbp]
\begin{align*}
  B_A &= A + \mathcal{P}_f(B_A) \times B_A \\
  D_A &= \mathcal{P}(B_A) \\
   \uplambda^E &: [D_A \to D_A] \to D_A \\
   \uplambda^E f &= \{ (D,d') \mid d' \in f\, D \} \\
   - \cdot_E - &: D_A \to D_A \to D_A \\
   D^{*}_1 \cdot_E D^{*}_2 &= \{ d' \mid \exists D, D \subseteq D^{*}_2 \land (D,d') \in D^{*}_1 \} \\[1ex]
\EE &: \mathbb{E} \to (\mathbb{X}\to D_A) \to D_A\\
\ESEM{ x }\rho &= \rho(x) \\
\ESEM{ (\LAM{x} e) }\rho &= \uplambda^E (\uplambda D^{*}.\, \ESEM{e}[x\mapsto D^{*}]\rho)\\
\ESEM{ (e_1\,e_2) }\rho &= (\ESEM{ e_1 }\rho) \cdot_E (\ESEM{ e_2 }\rho) 
\end{align*}
\caption{The $D_A$ model}
\label{fig:d-a}
\end{figure}


$D_A$ ordered by set inclusion $\subseteq$ is a cpo.


\begin{definition}
  A cpo $D$ is \emph{reflexive} if $[D \to D]$ is a retract of $D$.
  That is to say, there are continuous maps $F:D \to [D \to D]$ and
  $G: [D \to D] \to D $ such that for any $f \in [D \to D]$,
  \[
    (F \circ G)\, f = f
  \]
\end{definition}


\begin{lemma}
  $D_A$ is a reflexive cpo by choosing $F = \uplambda x y.\, x \cdot_E
  y$ and $G= \uplambda^E$. Therefore $D_A$ is a
  $\lambda$-model.
\end{lemma}
\begin{proof}
  We need to show that $(F \circ G)\,f = f$
  for any $f \in [D_A \to D_A]$.
  \begin{align*}
    (F \circ G)\,f &= \uplambda x. (\uplambda^E f) \cdot_E x\\
    &= \uplambda x. \{ d' \mid \exists D, D \subseteq x \land d' \in f\,D \}\\
    &= \uplambda x. \bigcup \{ f\,D \mid D \subseteq x \} \\
    &= \uplambda x.\, f\, x  & \text{by continuity of } f \\
    &= f
  \end{align*}
\end{proof}


\noindent Resources about $D_A$:
\begin{itemize}
\item \citet{barendregt84:_lambda_calculus} (Section 5.4),
\item \citet{Gunter:1992aa} (Section 8.1), and
\item \citet{Engeler:1981aa}.
\end{itemize}



%===============================================================================
\subsection{The $\mathcal{P}(\omega)$ model of \citet{Scott:1976lq}}
\label{sec:p-omega}


%% \marginnote{
%% \begin{tabular}{ll}
%% Reading: & \citet{barendregt84:_lambda_calculus} Sec. 18.1\\
%%          & \citet{Scott:1976lq}
%% \end{tabular}
%% }

Figure~\ref{fig:p-omega} defines a semantics for the
$\lambda$-calculus using the $\mathcal{P}(\omega)$ model of
\citet{Scott:1976lq}.  However, here we use the formulation of
$\mathcal{P}(\omega)$ given by \citet{barendregt84:_lambda_calculus}.

\newcommand{\EPo}[0]{\mathcal{E}_{\mathcal{P}(\omega)}}
\newcommand{\POSEM}[1]{\EPo{}\,#1\,}


\begin{figure}
\begin{align*}
     & m,n \in \mathit{Nat} \\
\langle n,m \rangle &= \frac{1}{2}(n + m)(n+m+1) + m \\
e_n &= \{ k_0, k_1, \ldots, k_{m-1} \} \quad \text{where }
    k_i < k_{i+1} \text{ and } n = \sum_{i<m} 2^{k_i}\\[2ex]
\EPo{} & : \mathbb{E} \to (\mathbb{X} \to \mathcal{P}(\mathit{Nat})) \to \mathcal{P}(\mathit{Nat}) \\
\POSEM{x}\rho &= \rho\,x \\  
\POSEM{(\LAM{x}e)}\rho &= \{ \langle n, m \rangle \mid m \in \POSEM{e}[x\mapsto e_n]\rho \} \\
\POSEM{(e_1\,e_2)}\rho &= \{ m \mid \exists e_n. \, 
   \langle n,m \rangle \in \POSEM{e_1}\rho
   \text{ and } e_n \subseteq \POSEM{e_2}\rho \}
\end{align*}
\caption{$\lambda$-calculus in $\mathcal{P}(\omega)$}
\label{fig:p-omega}
\end{figure}

%===============================================================================


\section{Filter models of $\lambda$-calculus}
\label{sec:filter-models}

UNDER CONSTRUCTION

\marginnote{
\begin{tabular}{ll}
Reading: & \cite{Alessi:2006aa}
\end{tabular}
}


\[
\begin{array}{lrl}
  \text{types} & A,B,C ::= & \mathtt{Nat} \mid \nu \mid A \to B \mid A \cap B
\end{array}
\]

Subtyping and equivalence relations
\[
\begin{array}{rlrl}
  \text{(refl)} &  A <: A & \text{(idem)} & A <: A \cap A \\[1ex]
  \text{(incl-L)} & A \cap B <: A & \text{(incl-R)} & A \cap B <: B\\[1ex]
  \text{(mon)} & \inference{A <: A' & B <: B'}{A \cap B <: A' \cap B'} &
  \text{(trans)} & \inference{A <: B & B <: C}{A <: C} \\[2ex]
  \text{($\nu$)} & A \to B <: \nu \\[1ex]
  \text{($\to-\cap^{\sim}$)} & (A \to B) \cap (A \to C) \sim A \to (B\cap C)&
  \text{($\eta^{\sim}$)} & \inference{A \sim A' & B \sim B'}
       {A \to B \sim A' \to B'} \\[1ex]
  & A \sim B \defeq A <: B \text{ and } B <: A
\end{array}
\]



\section{$D_\infty$ model of $\lambda$-calculus}
\label{sec:D-infinity}

UNDER CONSTRUCTION

\clearpage
\pagebreak

\section*{Answers to Exercises}

\shipoutAnswer

\clearpage
\pagebreak

\bibliographystyle{plainnat}
\bibliography{all}

\end{document}


\section{Records and Variants}

\section{Functions}

% STLC

Syntax
\[
\begin{array}{lrcl}
\text{expressions} & e & ::= & \ldots \mid x \mid \LAM{x\of T} e \mid e \APP e
\end{array}
\]


evaluation orders (specification)
\begin{itemize}
\item full $\beta$
\item CBV
\item CBN
\end{itemize}

\section{Recursive Functions}

% PCF
% statically typed, lambda, fix, nat, bool, if, primitives


\section{Dynamic Typing}


\section{The $\lambda$-calculus}

Syntax
\[
\begin{array}{lrcl}
\text{expressions} & e & ::= & \ldots \mid x \mid \LAM{x} e \mid e \APP e
\end{array}
\]



\section{Pointers}

\section{Miscellaneous}

A lub for a set $S$ can only exist of all the elements in $S$ are
consistent with one another.
%
\marginnote{$\{3\mapsto 8\}$ and $\{3 \mapsto 9\}$ are inconsistent.}
%
Consistency can be characterized purely in terms of the ordering
relation.

\begin{definition}
A subset $S$ of a partial order $(L,\sqsubseteq)$ is
\textbf{\emph{consistent}} if every pair of elements $x$ and $y$ in
$S$ has a common upper bound, i.e.  there exists $z \in L$ such $x
\sqsubseteq z$ and $y \sqsubseteq z$.
\end{definition}

In the poset of partial functions $(A \pto B, \subseteq)$,
every consistent subset $S$ has a lub, the function whose graph is the
union of all the graphs in $S$.

%
%% A partial order in which all consistent subsets have a least upper
%% bound is \textbf{\emph{consistently complete}}~\citep{Plotkin:1978aa}
%% (aka. \emph{bounded complete}~\citep{Gunter:1992aa}).

\begin{marginfigure}
\centering\xymatrix@=10pt{
 & \{ 1,2,3\} \ar@{-}[ddl]\ar@{-}[dr]\\
 & & \{ 2,3 \} \ar@{-}[dl]\ar@{-}[d]\\
\{1\}\ar@{-}[dr] & \{ 2 \} \ar@{-}[d]& \{ 3 \}\ar@{-}[dl]\\
   & \emptyset & 
} 
\caption{A directed subset of $(\mathcal{\mathbb{N}},\subseteq)$.}
\end{marginfigure}

However, a slightly different property, directedness, is used in the
fixed point theorem. (I am trying to find out why.) A directed set is
similar to a consistent set, but we only require an upper bound for
each pair in $S$ and the bound must itself be in $S$.

\begin{marginfigure}
\centering\xymatrix@=10pt{
 & & \{ 2,3 \} \ar@{-}[dl]\ar@{-}[d]\\
\{1\}\ar@{-}[dr] & \{ 2 \} \ar@{-}[d]& \{ 3 \}\ar@{-}[dl]\\
   & \emptyset & 
}
\caption{A subset of $(\mathcal{\mathbb{N}},\subseteq)$ that is
not directed.}
\end{marginfigure}


\begin{definition}
  Given a poset $(L,\sqsubseteq)$, a subset $S$ of $L$ is
  \textbf{\emph{directed}} if every pair of elements in $S$ has an
  upper bound in $S$.
\end{definition}

The sequence of approximations $\bot \sqsubseteq F(\bot) \sqsubseteq
F^2(\bot) \sqsubseteq \cdots$ is directed, any totally ordered subset
is, because the upper bound of two elements is just the larger
element.

\begin{marginfigure}
\centering\xymatrix@=10pt{
    & a \ar@{-}[dl]\ar@{-}[dr]& \\
  b &   & c
}
\caption{The subset $\{b,c\}$ is consistent but not directed.
  The subset $\{a,b,c\}$ is both consistent and directed.}
\end{marginfigure}

\begin{proposition}
  A finite directed set $S$ is also consistent.
\end{proposition}
\begin{proof}
  By induction on $S$. If $S=\emptyset$, then it vacuously consistent.
  Otherwise, remove a minimal element $x$ from $S$.  Then $S'=S -
  \{x\}$ is still directed, so $S'$ is consistent by the induction
  hypothesis, say with bound $z$. Then because $S$ is directed, for
  every $y \in S'$, the pair $x$ and $y$ have a bound $w$ in $S'$ (the
  bound couldn't be $x$), and $w \sqsubseteq z$ (because $S'$ is
  consistent) so $x \sqsubseteq z$.  Thus, $S$ is consistent.
\end{proof}

Generalizing from the poset of partial functions, one might think that
there is always a least upper bound of consistent or directed sets.
However, this is not always the case because their may not be enough
elements within the poset. For example, consider the poset
$(\mathit{Nat},\leq)$. The sequence
\[
  0 \leq 1 \leq 2 \leq 3 \leq \cdots
\]
does not have a lub in $(\mathit{Nat},\leq)$ because there is no one
natural number that is greater than all the rest. Of course, if we
move to the poset $(\mathit{Nat} \cup \{\infty\},\leq)$, where $n \leq
\infty$ for any $n \in \mathit{Nat}$, then $\infty$ is the lub of the
above sequence. So in the abstract setting of partial orders, one must
separately add the requirement that there exists a lub for any
directed subset.

\begin{marginfigure}
\centering\xymatrix@=10pt{
 a \ar@{-}[d]\ar@{-}[drr] &          & b \ar@{-}[d] \ar@{-}[dll]\\
 c \ar@{-}[dr]&          & d \ar@{-}[dl]\\
         & e 
}  \\[2ex]
\caption{A dcpo with a consistent set $\{c,d\}$ that has no
  lub~\citep{Plotkin:1978aa}.}
\end{marginfigure}

\begin{definition}
  A \textbf{\emph{directed-complete partial order (dcpo)}} has a least
  upper bound for every directed subset.
\end{definition}

\begin{proposition}[Partial functions are dcpos]
  Given any two sets $A$ and $B$, the partial order $(A \pto B,
  \subseteq)$, is a dcpo.
\end{proposition}

\begin{theorem}[Fixed Point Theorem for Dcpos]\label{thm:fixed-point}
Suppose $(L,\sqsubseteq)$ is a dcpo with a least element $\bot$, and
let $F:L\to L$ be a continuous function. Then $F$ has a least fixed
point, written $\mathrm{fix}\,F$, which is the least upper bound of
the ascending chain of $F$:
\[
  \mathrm{fix}\,F = \bigsqcup \{ F^n(\bot) \mid n \in \mathit{Nat} \}
\]
\end{theorem}
\begin{proof}
We first prove that $\mathrm{fix}\,F$ is a fixed point of $F$.
\begin{align*}
  F(\mathrm{fix}\,F) &= F(\bigsqcup \{ F^n(\bot) \mid n \in \NAT \})\\
  &= \bigsqcup\{ F(F^n(\bot)) \mid n \in \NAT \} & \text{by continuity}\\
  &= \bigsqcup\{ F^{n+1}(\bot)) \mid n \in \NAT \} \\
  %% &= \bigsqcup \{ F^1(\bot), F^2(\bot), F^3(\bot), \ldots\} \\
  %% &= \bigsqcup \{ F^0(\bot), F^1(\bot), F^2(\bot), F^3(\bot), \ldots\} \\
  &= \bigsqcup\{ F^n(\bot)) \mid n \in \NAT \} 
  & \text{because } F^0(\bot) = \bot \sqsubseteq F^1(\bot)  \\
  &= \mathrm{fix}\,F
\end{align*}
Next we prove that $\mathrm{fix}\,F$ is the least of the fixed points
of $F$. Suppose $e$ is an arbitrary fixed point of $F$. By the
monotonicity of $F$ we have $F^i(\bot) \sqsubseteq F^i(e)$ for all
$i$.  And because $e$ is a fixed point, we also have $F^i(e) = e$, so
$e$ is an upper bound of the ascending chain, and therefore
$\mathrm{fix}\,F \sqsubseteq e$.
\end{proof}


%%  LocalWords:  implementers
